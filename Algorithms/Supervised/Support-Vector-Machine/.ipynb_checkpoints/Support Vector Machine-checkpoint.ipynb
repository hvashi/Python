{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gathering Data\n",
    "credit = pd.read_csv(r\"E:/Github/Python/Data-Sets/Risk.txt\",sep=\",\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARITAL column unique values: ['married  ' 'single   ' 'divsepwid']\n",
      "HOWPAID column unique values: ['monthly' 'weekly ']\n",
      "MORTGAGE column unique values: ['y' 'n']\n",
      "GENDER column unique values: ['m' 'f']\n",
      "RISK column unique values: ['good risk ' 'bad loss  ' 'bad profit']\n"
     ]
    }
   ],
   "source": [
    "#Searching for unique values of string columns\n",
    "print('MARITAL column unique values:',credit['MARITAL'].unique())\n",
    "print('HOWPAID column unique values:',credit['HOWPAID'].unique())\n",
    "print('MORTGAGE column unique values:',credit['MORTGAGE'].unique())\n",
    "print('GENDER column unique values:',credit['GENDER'].unique())\n",
    "print('RISK column unique values:',credit['RISK'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Before modelling we need to compute new columns for string data\n",
    "credit['MARITAL_new'] = credit.MARITAL.map({'married  ':0, 'single   ':1,'divsepwid':2})\n",
    "credit['GENDER_new'] = credit.GENDER.map({'m':0, 'f':1})\n",
    "credit['MORTGAGE_new'] = credit.MORTGAGE.map({'y':0, 'n':1})\n",
    "credit['RISK_new'] = np.where(credit['RISK'].str.contains('good'), 1,0)\n",
    "credit['HOWPAID_new'] = credit.HOWPAID.map({'monthly':0, 'weekly ':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  AGE  INCOME GENDER    MARITAL  NUMKIDS  NUMCARDS  HOWPAID MORTGAGE  \\\n",
      "0  100756   44   59944      m  married          1         2  monthly        y   \n",
      "1  100668   35   59692      m  married          1         1  monthly        y   \n",
      "2  100418   34   59508      m  married          1         1  monthly        y   \n",
      "3  100416   34   59463      m  married          0         2  monthly        y   \n",
      "4  100590   39   59393      f  married          0         2  monthly        y   \n",
      "\n",
      "   STORECAR  LOANS        RISK  MARITAL_new  GENDER_new  MORTGAGE_new  \\\n",
      "0         2      0  good risk             0           0             0   \n",
      "1         1      0  bad loss              0           0             0   \n",
      "2         2      1  good risk             0           0             0   \n",
      "3         1      1  bad loss              0           0             0   \n",
      "4         1      0  good risk             0           1             0   \n",
      "\n",
      "   RISK_new  HOWPAID_new  \n",
      "0         1            0  \n",
      "1         0            0  \n",
      "2         1            0  \n",
      "3         0            0  \n",
      "4         1            0  \n"
     ]
    }
   ],
   "source": [
    "#Top 5 rows of dataframe\n",
    "print(credit.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              False\n",
       "AGE             False\n",
       "INCOME          False\n",
       "GENDER          False\n",
       "MARITAL         False\n",
       "NUMKIDS         False\n",
       "NUMCARDS        False\n",
       "HOWPAID         False\n",
       "MORTGAGE        False\n",
       "STORECAR        False\n",
       "LOANS           False\n",
       "RISK            False\n",
       "MARITAL_new     False\n",
       "GENDER_new      False\n",
       "MORTGAGE_new    False\n",
       "RISK_new        False\n",
       "HOWPAID_new     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null column check\n",
    "credit.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting X and Y variables \n",
    "X = credit[['AGE','INCOME','GENDER_new','MARITAL_new','NUMKIDS','NUMCARDS','HOWPAID_new','MORTGAGE_new','STORECAR','LOANS']]\n",
    "y = credit['RISK_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading train test split library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Creating X and y train and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Library\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying model\n",
    "svc.fit (X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "y_train_predictions = svc.predict(X_train)\n",
    "y_test_predictions = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report:\n",
    "\n",
    "Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures.\n",
    "\n",
    "The classification_report() function displays the precision, recall, f1-score and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.The best value is 1 and the worst value is 0.\n",
      "\n",
      "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.The best value is 1 and the worst value is 0.\n",
      "\n",
      "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall).\n",
      "\n",
      "The support is the number of occurrences of each class in y_true.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      1.00      0.88       651\n",
      "          1       0.00      0.00      0.00       173\n",
      "\n",
      "avg / total       0.62      0.79      0.70       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_test_predictions)\n",
    "print('The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.The best value is 1 and the worst value is 0.')\n",
    "print('')\n",
    "print('The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.The best value is 1 and the worst value is 0.')\n",
    "print('')\n",
    "print('The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall).')\n",
    "print('')\n",
    "print('The support is the number of occurrences of each class in y_true.')\n",
    "print('')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix:\n",
    "\n",
    "The confusion matrix is a handy presentation of the accuracy of a model with two or more classes.\n",
    "\n",
    "The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm.\n",
    "\n",
    "For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction=0 and actual=0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual=1. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "[[2662    0]\n",
      " [  16  615]]\n",
      "\n",
      "Test:\n",
      "\n",
      "[[651   0]\n",
      " [173   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print ('Train:\\n')\n",
    "print(confusion_matrix(y_train, y_train_predictions))\n",
    "print('')\n",
    "print ('Test:\\n')\n",
    "print(confusion_matrix(y_test, y_test_predictions))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy:\n",
    "\n",
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions made.\n",
    "\n",
    "This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.21)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = SVC()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. \n",
    "\n",
    "ROC can be broken down into sensitivity and specificity. A binary classification problem is really a trade-off between sensitivity and specificity.\n",
    "\n",
    "Sensitivity is the true positive rate also called the recall. It is the number instances from the positive (first) class that actually predicted correctly.\n",
    "\n",
    "Specificity is also called the true negative rate. Is the number of instances from the negative class (second) class that were actually predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = SVC()\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
