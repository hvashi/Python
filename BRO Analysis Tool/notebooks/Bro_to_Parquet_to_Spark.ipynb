{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/bro.png\" width=\"100px\"></div>\n",
    "\n",
    "# Bro to Parquet to Spark\n",
    "Apache Parquet is a columnar storage format focused on performance. Parquet data is often used within the Hadoop ecosystem and we will specifically be using it for loading data into both Pandas and Spark.\n",
    "\n",
    "<div style=\"float: right; margin: 30px -100px 0px 0px\"><img src=\"images/parquet.png\" width=\"300px\"></div>\n",
    "\n",
    "### Software\n",
    "- Bro Analysis Tools (BAT): https://github.com/Kitware/bat\n",
    "- Pandas: https://github.com/pandas-dev/pandas\n",
    "- Parquet: https://parquet.apache.org\n",
    "- Spark: https://spark.apache.org\n",
    "\n",
    "<div style=\"float: right; margin: 30px 0px 0px 0px\"><img src=\"images/spark.png\" width=\"200px\"></div>\n",
    "\n",
    "### Data\n",
    "- Sec Repo: http://www.secrepo.com (there's no Bro headers on these)\n",
    "- Kitware: https://data.kitware.com/#collection/58d564478d777f0aef5d893a (with headers)\n",
    "\n",
    "<div style=\"float: left; margin: 80px 20px 50px 20px\"><img src=\"images/bleeding.jpg\" width=\"250px\"></div>\n",
    "### Bleeding Edge Warning:\n",
    "You know you're on the bleeding edge when you link PRs that are still open/in-progess. There are **two open issues** with saving Parquet Files right now.\n",
    "\n",
    "- Timestamps in Spark: https://issues.apache.org/jira/browse/ARROW-1499\n",
    "- TimeDelta Support: https://issues.apache.org/jira/browse/ARROW-835\n",
    "\n",
    "For Spark timestamps, the BAT Parquet writer used below will output INT96 timestamps for now (we'll change over later when ARROW-1499 is complete). \n",
    "\n",
    "For the TimeDelta support we'll just have to wait until that gets pushed into the main branch and released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAT: 0.2.7\n",
      "PySpark: 2.2.0\n",
      "PyArrow: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "# Third Party Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow\n",
    "\n",
    "# Local imports\n",
    "import bat\n",
    "from bat.log_to_parquet import log_to_parquet\n",
    "\n",
    "# Good to print out versions of stuff\n",
    "print('BAT: {:s}'.format(bat.__version__))\n",
    "print('PySpark: {:s}'.format(pyspark.__version__))\n",
    "print('PyArrow: {:s}'.format(pyarrow.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bro log to Parquet File\n",
    "Here we're loading in a Bro HTTP log with ~2 million rows to demonstrate the functionality and do some simple spark processing on the data.\n",
    "- log_to_parquet is iterative so it can handle large files\n",
    "- 'row_group_size' defaults to 1 Million rows but can be set manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully monitoring /Users/briford/data/bro/sec_repo/http.log...\n",
      "Writing 0 rows...\n",
      "Writing 1000000 rows...\n",
      "Writing 2000000 rows...\n",
      "Writing 2048441 rows...\n",
      "Parquet File Complete\n"
     ]
    }
   ],
   "source": [
    "# Create a Parquet file from a Bro Log with a super nice BAT method.\n",
    "log_to_parquet('/Users/briford/data/bro/sec_repo/http.log', 'http.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/compressed.jpeg\" width=\"300px\"></div>\n",
    "\n",
    "# Parquet files are compressed\n",
    "Here we see the first benefit of Parquet which stores data with compressed columnar format. There are several compression options available (including uncompressed).\n",
    "\n",
    "## Original http.log = 1.3 GB \n",
    "## http.parquet = 106 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark.png\" width=\"200px\"></div>\n",
    "\n",
    "# Spark It!\n",
    "### Spin up Spark with 4 Parallel Executors\n",
    "Here we're spinning up a local spark server with 4 parallel executors, although this might seem a bit silly since we're probably running this on a laptop, there are a couple of important observations:\n",
    "\n",
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark_jobs.png\" width=\"400px\"></div>\n",
    "\n",
    "- If you have 4/8 cores use them!\n",
    "- It's the exact same code logic as if we were running on a distributed cluster.\n",
    "- We run the same code on **DataBricks** (www.databricks.com) which is awesome BTW.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spin up a local Spark Session (with 4 executors)\n",
    "spark = SparkSession.builder.master('local[4]').appName('my_awesome').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/fast.jpg\" width=\"350px\"></div>\n",
    "\n",
    "# Parquet files are fast\n",
    "We see from the below timer output that the Parquet file only takes a few seconds to read into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.74 ms, sys: 1.36 ms, total: 4.1 ms\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "# Have Spark read in the Parquet File\n",
    "%time spark_df = spark.read.parquet(\"http.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 0px 0px 0px -80px\"><img src=\"images/spark_distributed.png\" width=\"500px\"></div>\n",
    "\n",
    "# Parquet files are Parallel\n",
    "We see that, in this case, the number of data partitions in our dataframe(rdd) equals the number of executors/workers. If we had 8 workers there would be 8 partitions (at least, often there are more partitions based on how big the data is and how the files were writen, etc). \n",
    "\n",
    "\n",
    "**Image Credit:** Jacek Laskowski, please see his excellent book - Mastering Apache Spark  https://jaceklaskowski.gitbooks.io/mastering-apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; margin: 20px 20px 20px 20px\"><img src=\"images/eyeball.jpeg\" width=\"150px\"></div>\n",
    "# Lets look at our data\n",
    "We should always inspect out data when it comes in. Look at both the data values and the data types to make sure you're getting exactly what you should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 2048442\n",
      "Columns: filename,host,id.orig_h,id.orig_p,id.resp_h,id.resp_p,info_code,info_msg,method,orig_fuids,orig_mime_types,password,proxied,referrer,request_body_len,resp_fuids,resp_mime_types,response_body_len,status_code,status_msg,tags,trans_depth,uid,uri,user_agent,username,ts\n"
     ]
    }
   ],
   "source": [
    "# Get information about the Spark DataFrame\n",
    "num_rows = spark_df.count()\n",
    "print(\"Number of Rows: {:d}\".format(num_rows))\n",
    "columns = spark_df.columns\n",
    "print(\"Columns: {:s}\".format(','.join(columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+--------------+-----------+--------------------+\n",
      "|     id.orig_h|           host|           uri|status_code|          user_agent|\n",
      "+--------------+---------------+--------------+-----------+--------------------+\n",
      "|192.168.202.79|192.168.229.251|/DEASLog02.nsf|        404|Mozilla/5.0 (comp...|\n",
      "|192.168.202.79|192.168.229.251|/DEASLog03.nsf|        404|Mozilla/5.0 (comp...|\n",
      "|192.168.202.79|192.168.229.251|/DEASLog04.nsf|        404|Mozilla/5.0 (comp...|\n",
      "|192.168.202.79|192.168.229.251|/DEASLog05.nsf|        404|Mozilla/5.0 (comp...|\n",
      "|192.168.202.79|192.168.229.251|  /DEASLog.nsf|        404|Mozilla/5.0 (comp...|\n",
      "+--------------+---------------+--------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(['`id.orig_h`', 'host', 'uri', 'status_code', 'user_agent']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/fast.jpg\" width=\"350px\"></div>\n",
    "\n",
    "# Did we mention fast?\n",
    "The query below was executed on 4 workers. The data contains over 2 million HTTP requests/responses and the time to complete was **less than 1 second**. All this code is running on a 2016 Mac Laptop :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "| method|status_code|  count|\n",
      "+-------+-----------+-------+\n",
      "|   HEAD|        404|1294022|\n",
      "|    GET|        404| 429361|\n",
      "|   POST|        200| 125638|\n",
      "|    GET|        200|  88636|\n",
      "|   POST|          0|  32918|\n",
      "|    GET|        400|  29152|\n",
      "|    GET|        303|  10858|\n",
      "|    GET|        403|   8530|\n",
      "|   POST|        404|   4277|\n",
      "|    GET|        304|   3851|\n",
      "|    GET|        302|   3250|\n",
      "|    GET|          0|   2823|\n",
      "|    GET|        401|   2159|\n",
      "|OPTIONS|        200|   1897|\n",
      "|   POST|        302|   1226|\n",
      "|   HEAD|        503|   1010|\n",
      "|   POST|        206|    869|\n",
      "|    GET|        301|    642|\n",
      "|   HEAD|          0|    606|\n",
      "|    GET|        503|    550|\n",
      "+-------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 4.01 ms, sys: 1.53 ms, total: 5.54 ms\n",
      "Wall time: 848 ms\n"
     ]
    }
   ],
   "source": [
    "%time spark_df.groupby('method','status_code').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 50px 0px 0px 20px\"><img src=\"images/deep_dive.jpeg\" width=\"350px\"></div>\n",
    "\n",
    "# Data looks good, lets take a deeper dive\n",
    "Spark has a powerful SQL engine as well as a Machine Learning library. So now that we've got the data loaded into Parquet we're going to utilize the Spark SQL commands to do some investigation and clustering using the Spark MLLib. For this deeper dive we're going to go to another notebook :)\n",
    "\n",
    "### Spark Clustering Notebook\n",
    "- [Bro Spark Clustering](https://github.com/Kitware/bat/blob/master/notebooks/Spark_Clustering.ipynb)\n",
    "\n",
    "<div style=\"float: left; margin: 0px 0px 0px 0px\"><img src=\"images/spark_sql.jpg\" width=\"150px\"></div>\n",
    "<div style=\"float: left; margin: -20px 50px 0px 0px\"><img src=\"images/mllib.png\" width=\"150px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 50px 0px 0px -20px\"><img src=\"https://www.kitware.com/img/small_logo_over.png\" width=\"250px\"></div>\n",
    "## Wrap Up\n",
    "Well that's it for this notebook, we went from a Bro log to a high performance Parquet file and then did some digging with high speed, parallel SQL and groupby operations.\n",
    "\n",
    "If you liked this notebook please visit the [BAT](https://github.com/Kitware/bat) project for more notebooks and examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
