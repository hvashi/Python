{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/bro.png\" width=\"130px\"></div>\n",
    "\n",
    "# Bro to Kafka to Spark\n",
    "This notebook covers how to stream Bro data into Spark using Kafka as a message queue. The setup takes a bit of work but the result will be a nice scalable, robust way to process and analyze streaming data from Bro.\n",
    "\n",
    "For a super **EASY** way to get started with Spark (local data without Kafka) you can view this notebook:\n",
    "- https://github.com/Kitware/bat/blob/master/notebooks/Bro_to_Spark_Cheesy.ipynb\n",
    "\n",
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/kafka.png\" width=\"180px\"></div>\n",
    "\n",
    "### Software\n",
    "- Bro Network Monitor: https://www.bro.org\n",
    "- Kafka: https://kafka.apache.org\n",
    "- Spark: https://spark.apache.org\n",
    "\n",
    "<div style=\"float: right; margin: 0px 0px 0px 0px\"><img src=\"images/spark.png\" width=\"200px\"></div> \n",
    "\n",
    "<div style=\"float: left; margin: 20px 20px 20px 20px\"><img src=\"https://www.kitware.com/img/small_logo_over.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 0px 0px 0px\"><img src=\"images/confused.jpg\" width=\"300px\"></div>\n",
    "\n",
    "# Getting Everything Setup\n",
    "Get a nice cup of coffee and some snacks, we estimate 45-60 minutes for setting up a local Spark server and configuring Bro with the Kafka plugin. \n",
    "\n",
    "## Install local Spark Server\n",
    "For this notebook we're going to be using a local spark server, obviously we would want to setup a cluster for running Spark for a real system.\n",
    "```\n",
    "$ pip install pyspark\n",
    "```\n",
    "Yep, that's it. The latest version of PySpark will setup a local server with a simple pip install.\n",
    "\n",
    "# Setting up Bro with the Kafka Plugin\n",
    "So this is the most complicated part of the setup, once you've setup the Bro Kafka plugin you're basically done.\n",
    "\n",
    "## Shout Out/Thanks:\n",
    "- **Mike Sconzo** (for the initial setup instructions and general helpful awesomeness)\n",
    "- Jon Zeolla,  Nick Allen, and Casey Stella for the Kafka Plugin (Apache Metron project)\n",
    "- Kafka Plugin currently exists here:https://github.com/JonZeolla/metron-bro-plugin-kafka\n",
    "- The official Bro Package, when done, will be here: https://github.com/apache/metron-bro-plugin-kafka\n",
    "\n",
    "## Install Kafka\n",
    "```\n",
    "$ brew install kafka\n",
    "$ brew Install librdkafka\n",
    "$ brew install zookeeper\n",
    "```\n",
    "**Note: For Ubuntu 16.04 instructions see: https://hevo.io/blog/how-to-set-up-kafka-on-ubuntu-16-04/**\n",
    "\n",
    "## Add Kafka plugin to Bro\n",
    "**Note**: This will be much easier when the Kafka Plugin is a Bro 'Package' (coming soon :)\n",
    "\n",
    "**Get and Compile Bro (you have do this for now)**\n",
    "```\n",
    "$ git clone --recursive https://github.com/bro/bro.git\n",
    "$ cd bro \n",
    "$ ./configure\n",
    "$ make install\n",
    "```\n",
    "\n",
    "**Get the Kafka Bro plugin**\n",
    "```\n",
    "$ git clone https://github.com/JonZeolla/metron-bro-plugin-kafka\n",
    "$ cd metron-bro-plugin-kafka\n",
    "$ ./configure --bro-dist=$BRO_SRC_PATH\n",
    "$ make install\n",
    "```\n",
    "\n",
    "## Test the Bro Kafka Plugin\n",
    "```\n",
    "$ bro -N Bro::Kafka\n",
    "> Bro::Kafka - Writes logs to Kafka (dynamic, version 0.1)\n",
    "```\n",
    "\n",
    "## Setup plugin in local.bro\n",
    "Okay, so the logic below will output each log to a different Kafka topic. So the dns.log events will be sent to the 'dns' topic and the http.log events will be sent to the 'http' topic.. etc. If you'd like to send all the events to one topic or other configurations, please see https://github.com/JonZeolla/metron-bro-plugin-kafka\n",
    "\n",
    "    @load Bro/Kafka/logs-to-kafka.bro\n",
    "    redef Kafka::topic_name = \"\";\n",
    "    redef Kafka::logs_to_send = set(Conn::LOG, HTTP::LOG, DNS::LOG, SMTP::LOG);\n",
    "    redef Kafka::kafka_conf = table([\"metadata.broker.list\"] = \"localhost:9092\");\n",
    "\n",
    "## Start Kafka\n",
    "```\n",
    "$ zkserver start\n",
    "$ kafka-server-start\n",
    "```\n",
    "\n",
    "## Run Bro\n",
    "```\n",
    "$ bro -i en0 <path to>/local.bro\n",
    "or \n",
    "$ broctl deploy\n",
    "```\n",
    "\n",
    "## Verify messages are in the queue\n",
    "```\n",
    "$ kafka-console-consumer --bootstrap-server localhost:9092 --topic dns\n",
    "```\n",
    "**After a second or two.. you should start seeing DNS requests/replies coming out.. hit Ctrl-C after you see some.**\n",
    "```\n",
    "{\"ts\":1503513688.232274,\"uid\":\"CdA64S2Z6Xh555\",\"id.orig_h\":\"192.168.1.7\",\"id.orig_p\":58528,\"id.resp_h\":\"192.168.1.1\",\"id.resp_p\":53,\"proto\":\"udp\",\"trans_id\":43933,\"rtt\":0.02226,\"query\":\"brian.wylie.is.awesome.tk\",\"qclass\":1,\"qclass_name\":\"C_INTERNET\",\"qtype\":1,\"qtype_name\":\"A\",\"rcode\":0,\"rcode_name\":\"NOERROR\",\"AA\":false,\"TC\":false,\"RD\":true,\"RA\":true,\"Z\":0,\"answers\":[\"17.188.137.55\",\"17.188.142.54\",\"17.188.138.55\",\"17.188.141.184\",\"17.188.129.50\",\"17.188.128.178\",\"17.188.129.178\",\"17.188.141.56\"],\"TTLs\":[25.0,25.0,25.0,25.0,25.0,25.0,25.0,25.0],\"rejected\":false}\n",
    "```\n",
    "# If you made it this far you are done!\n",
    "<div style=\"float: left; margin: 20px 20px 20px 20px\"><img src=\"images/whew.jpg\" width=\"300px\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark.png\" width=\"200px\"></div>\n",
    "\n",
    "# Structured Streaming in Spark\n",
    "Structured Streaming is the new hotness with Spark. Michael Armbrust from DataBricks gave a great talk at Spark Summit 2017 on Structured Streaming:\n",
    "- https://www.youtube.com/watch?v=8o-cyjMRJWg\n",
    "\n",
    "There's also a good example on the DataBricks blog:\n",
    "- https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Always good to print out versions of libraries\n",
    "print('PySpark: {:s}'.format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark.png\" width=\"200px\"></div>\n",
    "\n",
    "# Spark It!\n",
    "### Spin up Spark with 4 Parallel Executors\n",
    "Here we're spinning up a local spark server with 4 parallel executors, although this might seem a bit silly since we're probably running this on a laptop, there are a couple of important observations:\n",
    "\n",
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark_jobs.png\" width=\"400px\"></div>\n",
    "\n",
    "- If you have 4/8 cores use them!\n",
    "- It's the exact same code logic as if we were running on a distributed cluster.\n",
    "- We run the same code on **DataBricks** (www.databricks.com) which is awesome BTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spin up a local Spark Session\n",
    "# Please note: the config line is an important bit,\n",
    "# readStream.format('kafka') won't work without it\n",
    "spark = SparkSession.builder.master('local[4]').appName('my_awesome')\\\n",
    "        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: -20px -20px -20px -20px\"><img src=\"images/arrow.png\" width=\"350px\"></div>\n",
    "\n",
    "# Sidebar: Apache Arrow is going to be Awesome\n",
    "For all kinds of reasons, multi-core pipelines, cross language storage, basically it will improve and enable flexible/performant data analysis and machine learning pipelines.\n",
    "- Apache Arros: https://arrow.apache.org\n",
    "- Spark to Pandas: http://arrow.apache.org/blog/2017/07/26/spark-arrow\n",
    "- JupyterCon 2017 Wes McKinney: https://www.youtube.com/watch?v=wdmf1msbtVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize the conversion to Pandas\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming data pipeline\n",
    "Our streaming data pipeline looks conceptually like this.\n",
    "<div style=\"margin: 20px 20px 20px 20px\"><img src=\"images/pipeline.png\" width=\"750px\"></div>\n",
    "- Kafka Plugin for Bro\n",
    "- ** Publish (provides a nice decoupled architecture) **\n",
    "- Pull/Subscribe to whatever feed you want (http, dns, conn, x509...)\n",
    "- ETL (Extract Transform Load) on the raw message data (parsed data with types)\n",
    "- Perform Filtering/Aggregation\n",
    "- Data Analysis and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUBSCRIBE: Setup connection to Kafka Stream \n",
    "raw_data = spark.readStream.format('kafka') \\\n",
    "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "  .option('subscribe', 'dns') \\\n",
    "  .option('startingOffsets', 'latest') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ETL: Hardcoded Schema for DNS records (do this better later)\n",
    "from pyspark.sql.types import StructType, StringType, BooleanType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, struct\n",
    "\n",
    "dns_schema = StructType() \\\n",
    "    .add('ts', StringType()) \\\n",
    "    .add('uid', StringType()) \\\n",
    "    .add('id.orig_h', StringType()) \\\n",
    "    .add('id.orig_p', IntegerType()) \\\n",
    "    .add('id.resp_h', StringType()) \\\n",
    "    .add('id.resp_p', IntegerType()) \\\n",
    "    .add('proto', StringType()) \\\n",
    "    .add('trans_id', IntegerType()) \\\n",
    "    .add('query', StringType()) \\\n",
    "    .add('qclass', IntegerType()) \\\n",
    "    .add('qclass_name', StringType()) \\\n",
    "    .add('qtype', IntegerType()) \\\n",
    "    .add('qtype_name', StringType()) \\\n",
    "    .add('rcode', IntegerType()) \\\n",
    "    .add('rcode_name', StringType()) \\\n",
    "    .add('AA', BooleanType()) \\\n",
    "    .add('TC', BooleanType()) \\\n",
    "    .add('RD', BooleanType()) \\\n",
    "    .add('RA', BooleanType()) \\\n",
    "    .add('Z', IntegerType()) \\\n",
    "    .add('answers', StringType()) \\\n",
    "    .add('TTLs', StringType()) \\\n",
    "    .add('rejected', BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ETL: Convert raw data into parsed and proper typed data\n",
    "parsed_data = raw_data \\\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), dns_schema).alias('data')) \\\n",
    "  .select('data.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FILTER/AGGREGATE: In this case a simple groupby operation\n",
    "group_data = parsed_data.groupBy('`id.orig_h`', 'qtype_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id.orig_h: string (nullable = true)\n",
      " |-- qtype_name: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# At any point in the pipeline you can see what you're getting out\n",
    "group_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming pipeline output to an in-memory table\n",
    "Now, for demonstration and discussion purposes, we're going to pull the end of the pipeline  back into memory to inspect the output. A couple of things to note explicitly here:\n",
    "\n",
    "- Writing a stream to memory is dangerous and should be done only on small data. Since this is aggregated output we know it's going to be small.\n",
    "\n",
    "- The queryName param used below will be the name of the in-memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the end of our pipeline and pull it into memory\n",
    "dns_count_memory_table = group_data.writeStream.format('memory') \\\n",
    "  .queryName('dns_counts') \\\n",
    "  .outputMode('complete') \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x106615e10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dns_count_memory_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; margin: 20px 20px 20px 20px\"><img src=\"images/dynamic.jpg\" width=\"350px\"></div>\n",
    "\n",
    "# Streaming Query/Table: Looking Deeper\n",
    "Note: The in-memory table above is **dynamic**. So as the streaming data pipeline continues to process data the table contents will change. Below we make two of the **same** queries and as more data streams in the results will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNS Query Counts = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id.orig_h</th>\n",
       "      <th>qtype_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172.16.133.136</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.16.133.136</td>\n",
       "      <td>AAAA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id.orig_h qtype_name  count\n",
       "0  172.16.133.136          A      1\n",
       "1  172.16.133.136       AAAA      1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Pandas Dataframe by querying the in memory table and converting\n",
    "dns_counts_df = spark.sql(\"select * from dns_counts\").toPandas()\n",
    "print('DNS Query Counts = {:d}'.format(len(dns_counts_df)))\n",
    "dns_counts_df.sort_values(ascending=False, by='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; margin: 0px 20px 0px 0px\"><img src=\"images/eyeball.jpeg\" width=\"100px\"></div>\n",
    "\n",
    "# Same Query with Updated Results\n",
    "Now we run the same query as above and since the streaming pipeline continues to process new incoming data the in-memory table will **dynamically** update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNS Query Counts = 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id.orig_h</th>\n",
       "      <th>qtype_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172.16.133.136</td>\n",
       "      <td>A</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172.16.176.184</td>\n",
       "      <td>PTR</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.16.220.140</td>\n",
       "      <td>PTR</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>192.17.98.96</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.16.159.32</td>\n",
       "      <td>PTR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>172.16.133.136</td>\n",
       "      <td>SRV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172.16.128.51</td>\n",
       "      <td>PTR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>172.16.133.136</td>\n",
       "      <td>AAAA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>172.16.134.234</td>\n",
       "      <td>PTR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id.orig_h qtype_name  count\n",
       "4  172.16.133.136          A     13\n",
       "3  172.16.176.184        PTR      6\n",
       "1  172.16.220.140        PTR      4\n",
       "6    192.17.98.96       None      4\n",
       "2   172.16.159.32        PTR      2\n",
       "7  172.16.133.136        SRV      2\n",
       "0   172.16.128.51        PTR      1\n",
       "5  172.16.133.136       AAAA      1\n",
       "8  172.16.134.234        PTR      1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Pandas Dataframe by querying the in memory table and converting\n",
    "dns_counts_df = spark.sql(\"select * from dns_counts\").toPandas()\n",
    "print('DNS Query Counts = {:d}'.format(len(dns_counts_df)))\n",
    "dns_counts_df.sort_values(ascending=False, by='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We should stop our streaming pipeline when we're done\n",
    "dns_count_memory_table.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "Well that's it for this notebook, we know this ended before we got to the **exciting** part of the streaming data pipeline. For this notebook we showed everything in the pipeline up to aggregation. In future notebooks we'll dive into the deep end of our pipeline and cover the data analysis and machine learning aspects of Spark.\n",
    "<div style=\"margin: 20px 20px 20px 20px\"><img src=\"images/pipeline.png\" width=\"750px\"></div>\n",
    "\n",
    "<div style=\"float: right; margin: 0px 0px 0px 0px\"><img src=\"https://www.kitware.com/img/small_logo_over.png\" width=\"200px\"></div>\n",
    "If you liked this notebook please visit the [bat](https://github.com/Kitware/bat) project for more notebooks and examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
